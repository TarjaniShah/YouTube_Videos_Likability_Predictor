{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATA EXTRACTION CODE:\n",
    "Logic for extracting various features is implemented entirely by us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES:\n",
    "    All API calls are referred from this source: https://developers.google.com/youtube/v3/docs/videos\n",
    "    Progress Bar code referenced from: \n",
    "        https://github.com/allenwang28/YouTube-Virality-Predictor/blob/master/scripts/get_last_video_count.ipynb\n",
    "    Emoticon Removal:\n",
    "        http://stackoverflow.com/a/13752628/6762004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = 'Your developers key'\n",
    "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "YOUTUBE_API_VERSION = 'v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOLLOWING CODE EXTRACTS VIDEOIDs AND STATISTICS FOR THE 'SEARCH TERM' and NUMBER OF PAGES GIVEN BY THE USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 1\n",
    "#FUNCTION: Performs keyword search for all the channels matching the keyword\n",
    "#returnslist of channelIDs\n",
    "def youtube_channel_search(options, num_of_pages):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    channelids = []  \n",
    "    #print(2)\n",
    "    for page in range(0,int(num_of_pages)):\n",
    "        if page==0:\n",
    "            #print(3)\n",
    "            search_response = youtube.search().list(\n",
    "                q=options.q,\n",
    "                type='channel',\n",
    "                part='id,snippet',\n",
    "                relevanceLanguage='en',\n",
    "                maxResults=options.max_results\n",
    "                ).execute()\n",
    "            #print(search_response)\n",
    "            next_page_token=search_response.get('nextPageToken')\n",
    "            for search_result in search_response.get('items', []):\n",
    "                #print(5)\n",
    "                if search_result['id']['kind'] == 'youtube#channel':\n",
    "                    channelids.append('%s' % (search_result['id']['channelId']))\n",
    "                    #print(6)\n",
    "\n",
    "        else:\n",
    "            if next_page_token is not None:\n",
    "                search_response = youtube.search().list(\n",
    "                    q=options.q,\n",
    "                    type='channel',\n",
    "                    pageToken=next_page_token,\n",
    "                    part='id,snippet',\n",
    "                    relevanceLanguage='en',\n",
    "                    maxResults=options.max_results\n",
    "                    ).execute()\n",
    "                next_page_token=search_response.get('nextPageToken')\n",
    "                for search_result in search_response.get('items', []):\n",
    "                    if search_result['id']['kind'] == 'youtube#channel':\n",
    "                        channelids.append('%s' % (search_result['id']['channelId']))\n",
    "\n",
    "    return channelids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 2\n",
    "#Function: Converts duration of videos to seconds\n",
    "#PURPOSE: since the duration format provided by youtube is of type PT1H1M1S\n",
    "def convert_to_seconds(duration):\n",
    "    h=0\n",
    "    m=0\n",
    "    s=0\n",
    "    if 'H' not in duration:\n",
    "        if 'M' not in duration:\n",
    "            if 'S' in duration:\n",
    "                s=duration.split('S')[0]\n",
    "        else:\n",
    "            m=duration.split('M')[0]\n",
    "            if 'S' in duration:\n",
    "                s=duration.split('M')[1].split('S')[0]\n",
    "    else:\n",
    "        h=duration.split('H')[0]\n",
    "        if 'M' not in duration:\n",
    "            if 'S' in duration:\n",
    "                s=duration.split('H')[1].split('S')[0]\n",
    "        else:\n",
    "            m=duration.split('H')[1].split('M')[0]\n",
    "            if 'S' in duration:\n",
    "                s=duration.split('H')[1].split('M')[1].split('S')[0]\n",
    "    timeinseconds=int(h)*3600+int(m)*60+int(s)\n",
    "    return timeinseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 3\n",
    "#FUNCTION: Converts date time string to date\n",
    "#PURPOSE: used by other methods for conversion to time\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "def convert_to_date(dt):\n",
    "    d=parse(dt)\n",
    "    return d.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 4\n",
    "#FUNCTION: Converts date time string to weeks\n",
    "#PURPOSE: for finding age of channel\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "def convert_to_weeks(dt):\n",
    "    d=parse(dt)\n",
    "    a=d.strftime('%m/%d/%Y')\n",
    "    x=datetime.strptime(a, '%m/%d/%Y').date()\n",
    "    y=datetime.now().date()\n",
    "    weeks=((y-x).days)/7\n",
    "    return int(weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 5\n",
    "#FUNCTION: to get playlistids\n",
    "#PURPOSE: fetches the upload playlist of the channel and gets channel statistics\n",
    "def youtube_channel_videos(channelids):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "    playlist_ids=[]\n",
    "    video_ids=[]\n",
    "    channelstats={}\n",
    "    channeldetails = youtube.channels().list(\n",
    "                         part=\"snippet,contentDetails,statistics\",\n",
    "                         id=channelids,\n",
    "                         ).execute()\n",
    "     \n",
    "    for i in channeldetails.get('items', []):\n",
    "        #print(i)\n",
    "        playlistid=i[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "        channelstats[playlistid]=[i['statistics']['viewCount'] if i['statistics'].get('viewCount') is not None else 0,\n",
    "                                    i['statistics']['subscriberCount'] if i['statistics'].get('subscriberCount') is not None else 0,\n",
    "                                    i['statistics']['videoCount'] if i['statistics'].get('videoCount') is not None else 0,\n",
    "                                    convert_to_weeks(i['snippet']['publishedAt']),playlistid]\n",
    "        if playlistid is not None:\n",
    "            playlist_ids.append(playlistid) ##Fetching playlist ids of all the channels\n",
    "    df= youtube_playlist_videos(playlist_ids,channelstats)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 6\n",
    "#FUNCTION: gets videos in playlist\n",
    "#PURPOSE: fetches list of videos in a given playlist\n",
    "import progressbar\n",
    "bar = progressbar.ProgressBar()\n",
    "def youtube_playlist_videos(playlist_ids,channelstats):\n",
    "    COLUMN_NAMES=['VideoID','Title','Description','Thumbnail','PublishedDate','CategoryID','ChannelID','ChannelTitle',\n",
    "                  'CHViewCount','CHSubscriberCount','CHAge','CHVideoCount','PlaylistID','Tags','Duration','Caption','ViewCount',\n",
    "                  'LikeCount','DislikeCount','FavouriteCount','CommentCount']\n",
    "    dfmain=pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "    for i in playlist_ids:\n",
    "        video_ids=[] \n",
    "        playlistdetails = youtube.playlistItems().list(\n",
    "                             part=\"snippet,contentDetails\",\n",
    "                             maxResults=49,\n",
    "                             playlistId=i\n",
    "                             ).execute()\n",
    "        for j in playlistdetails.get('items', []):\n",
    "            videoid=j[\"contentDetails\"][\"videoId\"]\n",
    "            if videoid is not None:\n",
    "                video_ids.append(videoid) ##Fetching playlist ids of all the channels  \n",
    "        df=youtube_videos_metadata(','.join(map(str, video_ids)),channelstats[i])\n",
    "\n",
    "        dfmain=dfmain.append(df)\n",
    "    return dfmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 7\n",
    "#FUNCTION: gets video stats\n",
    "#PURPOSE: Used to get video stats from videoids and stores the data into dataframe\n",
    "def youtube_videos_metadata(videoids,channelstats):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "    COLUMN_NAMES=['VideoID','Title','Description','Thumbnail','PublishedDate','CategoryID','ChannelID','ChannelTitle',\n",
    "                  'CHViewCount','CHSubscriberCount','CHAge','CHVideoCount','PlaylistID','Tags','Duration','Caption','ViewCount',\n",
    "                  'LikeCount','DislikeCount','FavouriteCount','CommentCount']\n",
    "    dfmain=pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    i=0\n",
    "    videos={}\n",
    "    prev_like=0\n",
    "    prev_dislike=0\n",
    "    prev_comm=0\n",
    "    prev_view=0\n",
    "    # Call the videos.list method to retrieve location details for each video.\n",
    "    video_response = youtube.videos().list(\n",
    "                    part='snippet,contentDetails,statistics',\n",
    "                    id=videoids\n",
    "                    ).execute()\n",
    "\n",
    "    for video_result in video_response.get('items', []):\n",
    "        dfmain.loc[i]=[video_result['id']\n",
    "                       ,video_result['snippet']['title']\n",
    "                       ,video_result['snippet']['description']\n",
    "                       ,video_result['snippet']['thumbnails']['default']['url']\n",
    "                       ,convert_to_date(str(video_result['snippet']['publishedAt']))\n",
    "                       ,video_result['snippet']['categoryId']\n",
    "                       ,video_result['snippet']['channelId']\n",
    "                       ,video_result['snippet']['channelTitle']\n",
    "                       ,channelstats[0]\n",
    "                       ,channelstats[1]\n",
    "                       ,channelstats[2]\n",
    "                       ,channelstats[3]\n",
    "                       ,channelstats[4]\n",
    "                       ,', '.join(map(str, video_result['snippet']['tags'])) if video_result['snippet'].get('tags') is not None else ''\n",
    "                       ,convert_to_seconds(str(video_result['contentDetails']['duration']).replace('PT',''))\n",
    "                       ,video_result['contentDetails']['caption']\n",
    "                       ,video_result['statistics']['viewCount'] if video_result['statistics'].get('viewCount') is not None else 0\n",
    "                       ,video_result['statistics']['likeCount'] if video_result['statistics'].get('likeCount') is not None else 0\n",
    "                       ,video_result['statistics']['dislikeCount'] if video_result['statistics'].get('dislikeCount') is not None else 0                      \n",
    "                       ,video_result['statistics']['favoriteCount'] if video_result['statistics'].get('favoriteCount') is not None else 0\n",
    "                       ,video_result['statistics']['commentCount'] if video_result['statistics'].get('commentCount') is not None else 0\n",
    "                      ]\n",
    "        i=int(i)+1\n",
    "    return dfmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 8\n",
    "#FUNCTION: Main method-Starting point\n",
    "#PURPOSE: Used to get input search term from user and then gets video stats after which calls the method to get comment sentiment\n",
    "#count\n",
    "import argparse\n",
    "search_term=input(\"Kindly enter search term :\")\n",
    "num_of_pages= input(\"Enter number of pages :\")\n",
    "vid_ids=''\n",
    "COLUMN_NAMES=['VideoID','Title','Description','Thumbnail','PublishedDate','CategoryID','ChannelID','ChannelTitle',\n",
    "                  'CHViewCount','CHSubscriberCount','CHAge','CHVideoCount','PlaylistID','Tags','Duration','Caption','ViewCount',\n",
    "                  'LikeCount','DislikeCount','FavouriteCount','CommentCount']\n",
    "dfmain1=pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--q', help='Search term', default=search_term)\n",
    "    parser.add_argument('--max-results', help='Max results', default=40)\n",
    "    args = parser.parse_args(args=[])\n",
    "    channelids=youtube_channel_search(args, num_of_pages)\n",
    "    channel_ids=''\n",
    "    a=0\n",
    "    start=0\n",
    "    end=49\n",
    "    playlistvidscaptions={}\n",
    "    len_cha=int(len(channelids)/50)\n",
    "    print(len_cha)\n",
    "    for x in range(a,len_cha+1): \n",
    "        print(x)\n",
    "        channel_ids=''\n",
    "        for index,val in enumerate(channelids[start:end]):  \n",
    "            if index==48:\n",
    "                channel_ids = channel_ids+','+val\n",
    "                start=start+49\n",
    "                end=end+49\n",
    "                break\n",
    "            else:\n",
    "                channel_ids = channel_ids+','+val\n",
    "        channelplaylistsvideos=youtube_channel_videos(str(channel_ids[1:]))  \n",
    "        dfmain1=dfmain1.append(channelplaylistsvideos)\n",
    "    writer = pd.ExcelWriter('FinalVideoList.xlsx')\n",
    "    dfmain1.to_excel(writer)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO EXTRACT COMMENT FOR EACH VIDEO AND APPLY SENTIMENT ANALYSIS ON THE COMMENT. \n",
    "THIS CODE MAINTAINS THE COUNT OF POSITIVE, NEGATIVE AND NEUTRAL COMMENTS FOR EACH VIDEO HENCE TAKES LOT OF TIME TO PROCESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 9\n",
    "#FUNCTION: Cleans text of emoticons\n",
    "def no_emoji(text):\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    text_no_emoji = RE_EMOJI.sub(r'', text)\n",
    "    return text_no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 10\n",
    "#FUNCTION: gets sentiment of text\n",
    "#PURPOSE: Used to get sentiment of comments of the video\n",
    "def get_comment_sentiment(text_emoji):\n",
    "    text_no_emoji=no_emoji(text_emoji)\n",
    "    if text_no_emoji is not None and text_no_emoji.strip()!='' and len(text_no_emoji)>3:\n",
    "        if TextBlob(text_no_emoji).detect_language()=='en':\n",
    "            blob = TextBlob(text_no_emoji)\n",
    "            if blob.sentiment.polarity > 0:\n",
    "                val = \"positive\"\n",
    "            elif blob.sentiment.polarity == 0:\n",
    "                val = \"neutral\"\n",
    "            else:\n",
    "                val = \"negative\"\n",
    "        else:\n",
    "            val=None\n",
    "    else:\n",
    "        val= None\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHOD 11\n",
    "#FUNCTION: gets video comments\n",
    "#PURPOSE: Used to get video comment sentiment count for 2 and 7 days and merges it with the dataframe passed as argument  \n",
    "import progressbar\n",
    "bar = progressbar.ProgressBar()\n",
    "def youtube_videos_comments(df):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                    developerKey=DEVELOPER_KEY)\n",
    "    COLUMN_NAMES=['VideoID','Comm_count_p7','Comm_count_ng7','Comm_count_n7','Comm_count_p30','Comm_count_ng30','Comm_count_n30']\n",
    "    dfvideocomments=pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    video_comments={}\n",
    "    channel_ids=[]\n",
    "    i=0\n",
    "    for index, row in bar(df.iterrows()):\n",
    "    # Call the videos.list method to retrieve location details for each video.\n",
    "        count_positive2=0;count_negative2=0;count_neutral2=0;count_positive7=0;count_negative7=0;count_neutral7=0;\n",
    "        try:\n",
    "            viddate=convert_to_date(row[\"PublishedDate\"])\n",
    "            comments = youtube.commentThreads().list(\n",
    "                         part=\"snippet\",\n",
    "                         videoId=row[\"VideoID\"],\n",
    "                         textFormat=\"plainText\"\n",
    "                         ).execute()\n",
    "            for item in comments[\"items\"]: \n",
    "                date=convert_to_date(item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"])\n",
    "                x=datetime.strptime(viddate, '%m/%d/%Y').date()\n",
    "                y=datetime.strptime(date, '%m/%d/%Y').date()\n",
    "                days=(y-x).days\n",
    "                if days <=7:\n",
    "                    comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    sentiment=get_comment_sentiment(comment)\n",
    "                    if sentiment=='positive':\n",
    "                        count_positive2=count_positive2+1\n",
    "                        count_positive7=count_positive7+1\n",
    "                    elif sentiment=='negative':\n",
    "                        count_negative2=count_negative2+1\n",
    "                        count_negative7=count_negative7+1\n",
    "                    elif sentiment=='neutral':\n",
    "                        count_neutral2=count_neutral2+1\n",
    "                        count_neutral7=count_neutral7+1\n",
    "                    else:\n",
    "                        continue\n",
    "                elif days<=30:\n",
    "                    comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    sentiment=get_comment_sentiment(comment)\n",
    "                    if sentiment=='positive':\n",
    "                        count_positive7=count_positive7+1\n",
    "                    elif sentiment=='negative':\n",
    "                        count_negative7=count_negative7+1\n",
    "                    elif sentiment=='neutral':\n",
    "                        count_neutral7=count_neutral7+1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            dfvideocomments.loc[i]=[item[\"snippet\"][\"videoId\"],count_positive7,count_negative7,count_neutral7,count_positive30,\n",
    "                                    count_negative30,count_neutral30]\n",
    "            i=int(i)+1\n",
    "        except:\n",
    "            continue\n",
    "    dfresult = pd.merge(df, dfvideocomments, how='inner', on=['VideoID', 'VideoID'])\n",
    "    return dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfvideolst = pd.read_excel('FinalVideoList.xlsx')\n",
    "dfvidcomms=youtube_videos_comments(dfvideolst)\n",
    "writer1 = pd.ExcelWriter('FinalVideoListCommentInfo.xlsx')\n",
    "dfvidcomms.to_excel(writer1)\n",
    "writer1.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO EXTRACT DETAILS OF PREVIOUS VIDEO PER VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfvidcomm = pd.read_excel('FinalVideoListCommentInfo_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfChannel_new = dfvidcomm[dfvidcomm.groupby('ChannelID')['ChannelID'].transform(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueChannelID = list(set(dfChannel_new['ChannelID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfChannel_new = dfChannel_new[dfChannel_new['ViewCount'] > 0]\n",
    "dfChannel_new[\"LikePerView\"]=dfChannel_new['LikeCount'] / dfChannel_new['ViewCount'].astype(np.float)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FUNCTION: Abstracts a subset of Channel and sorts it by video published date and assigns previous video stats to a given video\n",
    "import progressbar\n",
    "import numpy as np\n",
    "bar = progressbar.ProgressBar()\n",
    "df=pd.DataFrame()\n",
    "for channel in bar(uniqueChannelID):\n",
    "        #lst=pd.DataFrame({'PView' : 0,'PLike' : 0,'PComments' : 0,'PDislike' : 0,'PFavorite' : 0},index=['49'])\n",
    "        arr=np.array\n",
    "        dfPtemp=pd.DataFrame()\n",
    "        dftemp = dfChannel_new[dfChannel_new['ChannelID'] == channel]\n",
    "        dftemp['PublishedDate']=pd.to_datetime(dftemp.PublishedDate)\n",
    "        dftemp.sort_values(by='PublishedDate',inplace=True,ascending=False)\n",
    "        dftemp.reset_index(inplace=True)\n",
    "        arrVC=dftemp[\"ViewCount\"][1:]\n",
    "        arrLC=dftemp[\"LikeCount\"][1:]\n",
    "        arrCC=dftemp[\"CommentCount\"][1:]\n",
    "        arrDC=dftemp[\"DislikeCount\"][1:]\n",
    "        arrFC=dftemp[\"FavouriteCount\"][1:]\n",
    "        arrLPV=dftemp[\"LikePerView\"][1:]\n",
    "        arrPubDate=dftemp[\"PublishedDate\"][1:]\n",
    "        dfPtemp[\"PView\"]=arrVC\n",
    "        dfPtemp[\"PLike\"]=arrLC\n",
    "        dfPtemp[\"PComments\"]=arrCC\n",
    "        dfPtemp[\"PDislike\"]=arrDC\n",
    "        dfPtemp[\"PFavorite\"]=arrFC\n",
    "        dfPtemp[\"PLikePerView\"]=arrLPV\n",
    "        dfPtemp[\"PPublishedDate\"]=arrPubDate\n",
    "        dfPtemp.append([0,0,0,0,0,0,0])\n",
    "        dfPtemp.reset_index(inplace=True)\n",
    "        dffinal=pd.concat([dftemp, dfPtemp], axis=1,ignore_index=False)\n",
    "        df=pd.concat([df,dffinal])\n",
    "df.drop('index',inplace=True,axis=1)\n",
    "df.fillna(0,inplace=True)\n",
    "writer1 = pd.ExcelWriter('FinalVideoListCommentPrevInfo.xlsx')\n",
    "df.to_excel(writer1)\n",
    "writer1.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MAJOR CHALLENGE WHILE IMPLEMENTING THIS CODE WAS TO FETCH MAXIMUM AMOUNT OF VIDEO DETAILS IN ONE GO SINCE VIDEOIDs ARE UPDATED BY YOUTUBE ON DAILY BASIS, HENCE WE STARTED WITH FETCHING\n",
    "# CHANNELIDs ---> UPLOAD PLAYLISTIDs---> VIDEOIDs---> VIDEODETAILS--->COMMENTS(SENTIMENT ANALYSIS)--->PREVIOUS VIDEO DETAILS.\n",
    "\n",
    "BY EXECUTING ABOVE THREE MODULES OF LINES OF CODE WE GET ALL THE REQUIRED FEATURES AND CAN PROCEED TO EXPLORATORY DATA ANALYSIS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
